{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:22.574108Z",
     "start_time": "2020-11-29T01:24:22.562348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension(\"collapsible_headings/main\")\n",
       "utils.load_extension(\"hide_input/main\")\n",
       "utils.load_extension(\"autosavetime/main\")\n",
       "utils.load_extension(\"execute_time/ExecuteTime\")\n",
       "utils.load_extension(\"code_prettify/code_prettify\")\n",
       "utils.load_extension(\"scroll_down/main\")\n",
       "utils.load_extension(\"jupyter-js-widgets/extension\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension(\"collapsible_headings/main\")\n",
    "utils.load_extension(\"hide_input/main\")\n",
    "utils.load_extension(\"autosavetime/main\")\n",
    "utils.load_extension(\"execute_time/ExecuteTime\")\n",
    "utils.load_extension(\"code_prettify/code_prettify\")\n",
    "utils.load_extension(\"scroll_down/main\")\n",
    "utils.load_extension(\"jupyter-js-widgets/extension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:24.631449Z",
     "start_time": "2020-11-29T01:24:22.992465Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import TargetEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from nnet import ReadDataset, Net,ResNet\n",
    "import time\n",
    "from loss_functions import interval_score_loss\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "tic = time.time()\n",
    "from models.lgbm import (compute_metrics, preprocess)\n",
    "\n",
    "from tools.metrics import (\n",
    "    get_avg_volumes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:24.647678Z",
     "start_time": "2020-11-29T01:24:24.638826Z"
    }
   },
   "outputs": [],
   "source": [
    "def curation_post(data):\n",
    "    aux = data.copy()\n",
    "\n",
    "    # Save arrays\n",
    "    aux_low = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_low\n",
    "    aux_high = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_high\n",
    "    aux_index = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].index\n",
    "\n",
    "    # Modify\n",
    "    aux.loc[aux_index, \"pred_95_low\"] = aux_high\n",
    "    aux.loc[aux_index, \"pred_95_high\"] = aux_low\n",
    "    \n",
    "    \n",
    "    if aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape[0]>0:\n",
    "        print('If errors they should appear: ')\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "\n",
    "    preds_aux = np.mean([aux.pred_95_low, aux.pred_95_high], axis=0)\n",
    "    \n",
    "    aux['prediction'] = preds_aux\n",
    "    return aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:24.658800Z",
     "start_time": "2020-11-29T01:24:24.650739Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_metric(pred, lower, upper):\n",
    "\n",
    "    metric_pair = compute_metrics(\n",
    "        preds=pred,\n",
    "        lower=lower,\n",
    "        upper=upper,\n",
    "        y=val_y_raw,\n",
    "        offset=val_offset,\n",
    "        X=val_x_orig,\n",
    "        avg_volumes=avg_volumes,\n",
    "    )\n",
    "    return metric_pair[0],metric_pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:24.673671Z",
     "start_time": "2020-11-29T01:24:24.663366Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_submission(submission_df, solve_submission_issues=True):\n",
    "\n",
    "    join_on = [\"country\", \"brand\", \"month_num\"]\n",
    "    keep = join_on + [\"volume\"]\n",
    "\n",
    "    df_vol = pd.read_csv(\"../data/gx_volume.csv\").loc[:, keep]\n",
    "\n",
    "    both_ds = submission_df.merge(\n",
    "        df_vol,\n",
    "        on=join_on,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"prediction\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"pred_95_high\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values + 0.01\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"pred_95_low\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values - 0.01\n",
    "\n",
    "    final_cols = join_on + [\"pred_95_low\", \"prediction\", \"pred_95_high\"]\n",
    "\n",
    "    final_df =  both_ds.loc[:, final_cols]\n",
    "\n",
    "    if solve_submission_issues:\n",
    "\n",
    "        if (final_df.pred_95_low > final_df.pred_95_high).any():\n",
    "            raise(\"Stop please, upper < lower\")\n",
    "\n",
    "        cond_lower_mean = final_df.pred_95_low > final_df.prediction\n",
    "        if cond_lower_mean.any():\n",
    "            print(\"Solving lower > mean\")\n",
    "            final_df.loc[cond_lower_mean, \"prediction\"] = \\\n",
    "                final_df.loc[cond_lower_mean, \"pred_95_low\"] + 0.01\n",
    "\n",
    "        cond_upper_mean = final_df.prediction > final_df.pred_95_high\n",
    "        if cond_upper_mean.any():\n",
    "            print(\"Solving upper < mean\")\n",
    "            final_df.loc[cond_upper_mean, \"prediction\"] = \\\n",
    "                final_df.loc[cond_upper_mean, \"pred_95_high\"] - 0.01\n",
    "\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:24.681835Z",
     "start_time": "2020-11-29T01:24:24.677561Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_metric(pred, lower, upper):\n",
    "\n",
    "    metric_pair = compute_metrics(\n",
    "        preds=pred,\n",
    "        lower=lower,\n",
    "        upper=upper,\n",
    "        y=val_y_raw,\n",
    "        offset=val_offset,\n",
    "        X=val_x_orig,\n",
    "        avg_volumes=avg_volumes,\n",
    "    )\n",
    "    return metric_pair[0],metric_pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:25.240105Z",
     "start_time": "2020-11-29T01:24:24.742532Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset_name = \"last_before_3_after_0\"\n",
    "\n",
    "file_name = \"linear_base\"\n",
    "\n",
    "full_df = pd.read_csv(\"../data/gx_merged_lags_months.csv\")\n",
    "# volume_features = pd.read_csv(\"data/volume_features.csv\")\n",
    "submission_df = pd.read_csv(\"../data/submission_template.csv\")\n",
    "train_tuples = pd.read_csv(\"../data/train_split.csv\")\n",
    "valid_tuples = pd.read_csv(\"../data/valid_split.csv\")\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_df = full_df.merge(volume_features, on=[\"country\", \"brand\"])\n",
    "\n",
    "full_df[\"volume_offset\"] = (full_df[\"volume\"] - full_df[offset_name]) / full_df[offset_name]\n",
    "full_df = preprocess(full_df)\n",
    "\n",
    "test_df = full_df[full_df.test == 1].copy().reset_index(drop=True)\n",
    "\n",
    "real_full = full_df.copy()\n",
    "\n",
    "full_df = full_df[full_df.test == 0]\n",
    "\n",
    "train_df = full_df.merge(train_tuples, how=\"inner\").reset_index(drop=True)\n",
    "val_df = full_df.merge(valid_tuples, how=\"inner\").reset_index(drop=True)\n",
    "\n",
    "    # TODO: no need for calculation every time\n",
    "avg_volumes = get_avg_volumes()\n",
    "\n",
    "to_drop = [\"volume\", \"volume_offset\",'test','A','country_mean_before_24_after_0']\n",
    "categorical_cols = [\n",
    "        \"country\", \"brand\", \"therapeutic_area\", \"presentation\", \"month_name\",\n",
    "        \"month_country\", \"month_presentation\", \"month_area\",\n",
    "        \"month_country_num\", \"month_presentation_num\", \"month_area_num\",\n",
    "        \"month_month_num\"\n",
    "    ]\n",
    "\n",
    "    # Prep data\n",
    "train_x = train_df.drop(columns=to_drop)\n",
    "train_y = train_df.volume_offset\n",
    "train_offset = train_df[offset_name]\n",
    "\n",
    "\n",
    "\n",
    "full_x = full_df.drop(columns=to_drop)\n",
    "full_y = full_df.volume_offset\n",
    "full_offset = full_df[offset_name]\n",
    "\n",
    "real_full_x = real_full.drop(columns=to_drop)\n",
    "\n",
    "\n",
    "val_x = val_df.drop(columns=to_drop)\n",
    "val_x_orig = val_x\n",
    "val_y = val_df.volume_offset\n",
    "val_y_raw = val_df.volume\n",
    "val_offset = val_df[offset_name]\n",
    "\n",
    "test_x = test_df.drop(columns=to_drop)\n",
    "test_offset = test_df[offset_name]\n",
    "\n",
    "\n",
    "\n",
    "ccoo = [\n",
    "        \"mean_before_Inf_after_0\",\n",
    "        \"median_before_24_after_0\",\n",
    "        \"brand_median_before_Inf_after_0\",\n",
    "        \"median_before_Inf_after_0\",\n",
    "        \"country_mean_before_Inf_after_0\",\n",
    "        \"brand_mean_before_Inf_after_0\",\n",
    "        \"mean_before_3_after_0\",\n",
    "        \"median_before_12_after_0\",\n",
    "        \"mean_before_24_after_0\",\n",
    "        \"brand_mean_before_3_after_0\",\n",
    "        \"country_mean_before_3_after_0\",\n",
    "        \"brand_median_before_12_after_0\",\n",
    "        \"brand_mean_before_24_after_0\",\n",
    "        \"median_before_3_after_0\",\n",
    "        \"mean_before_12_after_0\",\n",
    "        \"brand_mean_before_12_after_0\",\n",
    "        \"brand_median_before_24_after_0\",\n",
    "        \"brand_median_before_3_after_0\",\n",
    "        \"B\",\n",
    "        \"D\",\n",
    "        \"brand_last_before_Inf_after_0\",\n",
    "        \"country_mean_before_12_after_0\",\n",
    "        \"brand_last_before_24_after_0\",\n",
    "        \"num_generics\",\n",
    "        \"brand_last_before_3_after_0\",\n",
    "        \"brand_last_before_12_after_0\",\n",
    "        \"last_before_3_after_0\",\n",
    "        \"last_before_24_after_0\",\n",
    "        \"C\",\n",
    "        'month_name','last_before_12_after_0','last_before_Inf_after_0','country_last_before_24_after_0',\n",
    "    'country_median_before_24_after_0','country_last_before_12_after_0',\n",
    "'country_median_before_12_after_0','country_last_before_Inf_after_0','country_median_before_Inf_after_0','country_last_before_3_after_0',\n",
    "'country_median_before_3_after_0','month_country']\n",
    "\n",
    "train_x = train_x.drop(\n",
    "    columns=ccoo\n",
    ")\n",
    "\n",
    "full_x = full_x.drop(\n",
    "    columns=ccoo\n",
    ")\n",
    "\n",
    "val_x = val_x.drop(\n",
    "    columns=ccoo\n",
    ")\n",
    "\n",
    "test_x = test_x.drop(\n",
    "    columns=ccoo\n",
    ")\n",
    "\n",
    "real_full_x = real_full_x.drop(\n",
    "    columns=ccoo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:27.899828Z",
     "start_time": "2020-11-29T01:24:27.642132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TargetEncoder(cols=['country', 'brand', 'therapeutic_area', 'presentation',\n",
       "                    'month_presentation', 'month_area', 'month_country_num',\n",
       "                    'month_presentation_num', 'month_area_num',\n",
       "                    'month_month_num'],\n",
       "              drop_invariant=False, handle_missing='value',\n",
       "              handle_unknown='value', min_samples_leaf=1, return_df=True,\n",
       "              smoothing=1.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "categorical_cols = [\n",
    "        \"country\", \"brand\", \"therapeutic_area\", \"presentation\", #\"month_name\",\"month_country\",\n",
    "         \"month_presentation\", \"month_area\",\n",
    "        \"month_country_num\", \"month_presentation_num\", \"month_area_num\",\n",
    "        \"month_month_num\"\n",
    "    ]\n",
    "te = TargetEncoder(cols=categorical_cols)\n",
    "te.fit(full_x,full_y)\n",
    "#te.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:28.118721Z",
     "start_time": "2020-11-29T01:24:27.903839Z"
    }
   },
   "outputs": [],
   "source": [
    "full_x = te.transform(full_x)\n",
    "train_x = te.transform(train_x)\n",
    "val_x = te.transform(val_x)\n",
    "test_x= te.transform(test_x)\n",
    "\n",
    "real_full_x =te.transform(real_full_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:28.464763Z",
     "start_time": "2020-11-29T01:24:28.457470Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = GaussRankScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:28.845970Z",
     "start_time": "2020-11-29T01:24:28.815119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussRankScaler(copy=True, epsilon=0.0001, interp_copy=False,\n",
       "                interp_kind='linear', n_jobs=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler.fit(train_x)\n",
    "scaler.fit(real_full_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:29.437416Z",
     "start_time": "2020-11-29T01:24:29.363424Z"
    }
   },
   "outputs": [],
   "source": [
    "full_x = pd.DataFrame(scaler.transform(full_x),columns=full_x.columns)\n",
    "train_x = pd.DataFrame(scaler.transform(train_x),columns=full_x.columns)\n",
    "val_x = pd.DataFrame(scaler.transform(val_x),columns=full_x.columns)\n",
    "test_x = pd.DataFrame(scaler.transform(test_x),columns=full_x.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:30.305140Z",
     "start_time": "2020-11-29T01:24:30.282566Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReadDataset(Dataset):\n",
    "    \"\"\"Read dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, XX,yy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with the students data.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = XX\n",
    "        self.y = yy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __shape__(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        self.X.iloc[idx].values\n",
    "        self.y[idx]\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:31.493786Z",
     "start_time": "2020-11-29T01:24:31.409480Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainset = ReadDataset(train_x,train_y)\n",
    "testset = ReadDataset(val_x,val_y)\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "# Test set\n",
    "\n",
    "X_train = torch.tensor(trainset.X.values)\n",
    "y_train = torch.tensor(trainset.y)\n",
    "\n",
    "X_test = torch.tensor(testset.X.values)\n",
    "y_test = torch.tensor(testset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:31.798526Z",
     "start_time": "2020-11-29T01:24:31.793201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:32.238986Z",
     "start_time": "2020-11-29T01:24:32.231257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "nnet = ResNet(trainset.__shape__()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:24:33.250661Z",
     "start_time": "2020-11-29T01:24:33.246546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    nnet.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08,  # weight_decay=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:26:15.973317Z",
     "start_time": "2020-11-29T01:24:34.103216Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(43.552089953116145, 261.5434641919407)\n",
      "5\n",
      "(19.255839166569313, 74.12990524594984)\n",
      "10\n",
      "(14.222908862970579, 48.367474363974644)\n",
      "15\n",
      "(12.605428962716358, 44.341068506031014)\n",
      "20\n",
      "(12.423439086566175, 44.23612495491755)\n",
      "25\n",
      "(12.386120022123933, 44.09601116489373)\n",
      "Elapsed time:  111.41897702217102\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Train the net\n",
    "loss_per_iter = []\n",
    "loss_per_batch = []\n",
    "\n",
    "\n",
    "# Train the net\n",
    "losses = []\n",
    "auc_train = []\n",
    "auc_test = []\n",
    "metric_val = []\n",
    "unc_val = []\n",
    "\n",
    "# hyperparameteres\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        X = inputs.to(device)\n",
    "        y = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forwarde\n",
    "        outputs = nnet(X.float())\n",
    "\n",
    "        # Compute diff\n",
    "\n",
    "        loss = interval_score_loss(outputs, y.float())\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save loss to plot\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(epoch)\n",
    "        auc_train.append(loss.cpu().detach().numpy())\n",
    "        pred = nnet(X_test.float())\n",
    "        auc_test.append(interval_score_loss(pred, y_test.float()))\n",
    "\n",
    "        preds = torch.mean(nnet(X_test.float()), axis=1).cpu().detach().numpy()\n",
    "        lower = nnet(X_test.float())[:, 0].cpu().detach().numpy()\n",
    "        upper = nnet(X_test.float())[:, 1].cpu().detach().numpy()\n",
    "        res = my_metric(preds, lower, upper)\n",
    "        print(res)\n",
    "        metric_val.append(res[0])\n",
    "        unc_val.append(res[1])\n",
    "\n",
    "        # Figure\n",
    "        plt.figure()\n",
    "        plt.plot(auc_train, label=\"train\")\n",
    "        plt.plot(auc_test, label=\"test\")\n",
    "        plt.plot(metric_val, label=\"Metric\")\n",
    "        plt.plot(unc_val, label=\"Uncertainty\")\n",
    "        plt.legend()\n",
    "        plt.ylim([0, 100])\n",
    "        plt.savefig(\"output/auc_NN.png\")\n",
    "        plt.savefig(\"output/auc_NN.svg\", format=\"svg\")\n",
    "        plt.close()\n",
    "\n",
    "        #\n",
    "        path = \"output/weights\" + str(epoch) + \".pt\"\n",
    "        torch.save(nnet.state_dict(), path)\n",
    "\n",
    "print(\"Elapsed time: \", np.abs(tic - time.time()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0\n",
    "(38.30321899645797, 185.9348884440183)\n",
    "5\n",
    "(21.203758668088536, 70.34309363777196)\n",
    "10\n",
    "(15.179473610528667, 51.18191842002765)\n",
    "15\n",
    "(13.316246371168205, 46.00865765964492)\n",
    "20\n",
    "(12.67028439561099, 43.98683043317141)\n",
    "25\n",
    "(12.459823880944969, 43.23603694361239)\n",
    "Elapsed time:  168.72150492668152\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:31.280643Z",
     "start_time": "2020-11-29T01:38:31.268681Z"
    }
   },
   "outputs": [],
   "source": [
    "def curation_post(data):\n",
    "    aux = data.copy()\n",
    "\n",
    "    # Save arrays\n",
    "    aux_low = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_low\n",
    "    aux_high = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_high\n",
    "    aux_index = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].index\n",
    "\n",
    "    # Modify\n",
    "    aux.loc[aux_index, \"pred_95_low\"] = aux_high\n",
    "    aux.loc[aux_index, \"pred_95_high\"] = aux_low\n",
    "    \n",
    "    \n",
    "    if aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape[0]>0:\n",
    "        print('If errors they should appear: ')\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "\n",
    "    preds_aux = np.mean([aux.pred_95_low, aux.pred_95_high], axis=0)\n",
    "    \n",
    "    aux['prediction'] = preds_aux\n",
    "    return aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:31.605862Z",
     "start_time": "2020-11-29T01:38:31.596157Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_withNN(NN, data):\n",
    "    # Make predictions\n",
    "    preds = torch.mean(NN(X_test.float()), axis=1).detach().numpy()\n",
    "    lower = NN(X_test.float())[:, 0].detach().numpy()\n",
    "    upper = NN(X_test.float())[:, 1].detach().numpy()\n",
    "\n",
    "    print(my_metric(preds, lower, upper))\n",
    "\n",
    "    # Modify offset\n",
    "    \n",
    "    #preds = (preds + 1) * val_offset\n",
    "    #lower = (lower + 1) * val_offset\n",
    "    #upper = (upper + 1) * val_offset\n",
    "\n",
    "    aux_data = data.copy()\n",
    "\n",
    "    aux_data[\"prediction\"] = preds\n",
    "    aux_data[\"pred_95_low\"] = lower\n",
    "    aux_data[\"pred_95_high\"] = upper\n",
    "\n",
    "    aux_data = curation_post(aux_data)\n",
    "    return aux_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:32.012371Z",
     "start_time": "2020-11-29T01:38:31.995395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet60 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet60.load_state_dict(torch.load(\"output/weights60.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:32.298536Z",
     "start_time": "2020-11-29T01:38:32.283346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet80 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet80.load_state_dict(torch.load(\"output/weights80.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:32.674121Z",
     "start_time": "2020-11-29T01:38:32.659008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet95 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet95.load_state_dict(torch.load(\"output/weights95.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:32.989956Z",
     "start_time": "2020-11-29T01:38:32.978131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet100 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet100.load_state_dict(torch.load(\"output/weights100.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:33.297473Z",
     "start_time": "2020-11-29T01:38:33.282687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet120 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet120.load_state_dict(torch.load(\"output/weights140.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:33.714434Z",
     "start_time": "2020-11-29T01:38:33.702633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet125 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet125.load_state_dict(torch.load(\"output/weights125.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T14:46:00.158649Z",
     "start_time": "2020-11-28T14:46:00.142701Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T14:47:03.603639Z",
     "start_time": "2020-11-28T14:47:03.590667Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:38.242484Z",
     "start_time": "2020-11-29T01:38:34.552153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.176386917141427, 42.93847216454228)\n",
      "(12.118290107326827, 42.3008156123991)\n",
      "(12.045534618465448, 41.98208389423535)\n",
      "(12.040466252283052, 41.7575059776463)\n"
     ]
    }
   ],
   "source": [
    "n_60 = predict_withNN(nnet60,val_x_orig[['country','brand','month_num']])\n",
    "n_80 = predict_withNN(nnet80,val_x_orig[['country','brand','month_num']])\n",
    "n_95 = predict_withNN(nnet95,val_x_orig[['country','brand','month_num']])\n",
    "n_100 = predict_withNN(nnet100,val_x_orig[['country','brand','month_num']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:38.273446Z",
     "start_time": "2020-11-29T01:38:38.245488Z"
    }
   },
   "outputs": [],
   "source": [
    "n_val_final = n_100.copy()\n",
    "\n",
    "n_val_final.prediction = np.mean([n_60.prediction,\n",
    "                                n_80.prediction,\n",
    "                                n_95.prediction,\n",
    "                                n_100.prediction],axis=0)\n",
    "\n",
    "n_val_final.pred_95_low = np.mean([n_60.pred_95_low,\n",
    "                                n_80.pred_95_low,\n",
    "                                n_95.pred_95_low,\n",
    "                                n_100.pred_95_low],axis=0)\n",
    "\n",
    "n_val_final.pred_95_high = np.mean([n_60.pred_95_high,\n",
    "                                n_80.pred_95_high,\n",
    "                                n_95.pred_95_high,\n",
    "                                n_100.pred_95_high],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:39.257081Z",
     "start_time": "2020-11-29T01:38:38.281889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.08819118412063, 42.17981392555193)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_metric(n_val_final.prediction,\n",
    "         n_val_final.pred_95_low,\n",
    "         n_val_final.pred_95_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:39.300239Z",
     "start_time": "2020-11-29T01:38:39.260326Z"
    }
   },
   "outputs": [],
   "source": [
    "n_val_final.to_csv('output/valid_ensemble.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:39.356462Z",
     "start_time": "2020-11-29T01:38:39.302577Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = torch.mean(nnet(X_test.float()),axis=1).detach().numpy()\n",
    "lower = nnet125(X_test.float())[:, 0].detach().numpy()\n",
    "upper = nnet125(X_test.float())[:, 1].detach().numpy()\n",
    "\n",
    "preds = (preds+1)*val_offset\n",
    "lower = (lower+1)*val_offset\n",
    "upper = (upper+1)*val_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:39.367934Z",
     "start_time": "2020-11-29T01:38:39.362525Z"
    }
   },
   "outputs": [],
   "source": [
    "val_preds = val_x\n",
    "\n",
    "val_preds[\"prediction\"] = preds\n",
    "val_preds[\"pred_95_low\"] = lower\n",
    "val_preds[\"pred_95_high\"] = upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:40.568377Z",
     "start_time": "2020-11-29T01:38:40.551804Z"
    }
   },
   "outputs": [],
   "source": [
    "def submission_predict(NN):\n",
    "\n",
    "    submission_df = pd.read_csv(\"../data/submission_template.csv\")\n",
    "\n",
    "    preds = torch.mean(NN(torch.tensor(test_x.values).float()), axis=1).detach().numpy()\n",
    "    lower = NN(torch.tensor(test_x.values).float())[:, 0].detach().numpy()\n",
    "    upper = NN(torch.tensor(test_x.values).float())[:, 1].detach().numpy()\n",
    "\n",
    "    preds = (preds + 1) * test_offset\n",
    "    lower = (lower + 1) * test_offset\n",
    "    upper = (upper + 1) * test_offset\n",
    "\n",
    "    submission_df[\"pred_95_low\"] = np.maximum(lower, 0)\n",
    "    submission_df[\"pred_95_high\"] = np.maximum(upper, 0)\n",
    "    submission_df[\"prediction\"] = np.maximum(preds, 0)\n",
    "    submission_df = curation_post(submission_df)\n",
    "\n",
    "\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"pred_95_high\"] < submission_df[\"pred_95_low\"]\n",
    "    ].shape[0]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"pred_95_low\"] > submission_df[\"pred_95_high\"]\n",
    "    ].shape[0]\n",
    "    print(submission_df[\n",
    "        submission_df[\"pred_95_low\"] > submission_df[\"pred_95_high\"]\n",
    "    ])\n",
    "    \n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"prediction\"] > submission_df[\"pred_95_high\"]\n",
    "    ].shape[0]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[submission_df[\"prediction\"] < submission_df[\"pred_95_low\"]].shape[\n",
    "        0\n",
    "    ]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "        \n",
    "        \n",
    "    submission_df = postprocess_submission(submission_df)\n",
    "\n",
    "    submission_df[\"pred_95_low\"] = np.maximum(submission_df.pred_95_low, 0)\n",
    "    submission_df[\"pred_95_high\"] = np.maximum(submission_df.pred_95_high, 0)\n",
    "    submission_df[\"prediction\"] = np.maximum(submission_df.prediction, 0)\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:41.617549Z",
     "start_time": "2020-11-29T01:38:41.378314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [country, brand, month_num, pred_95_low, prediction, pred_95_high]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [country, brand, month_num, pred_95_low, prediction, pred_95_high]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [country, brand, month_num, pred_95_low, prediction, pred_95_high]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [country, brand, month_num, pred_95_low, prediction, pred_95_high]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "pred60 = submission_predict(nnet60)\n",
    "pred80 = submission_predict(nnet80)\n",
    "pred95 = submission_predict(nnet95)\n",
    "pred100 = submission_predict(nnet100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:42.239112Z",
     "start_time": "2020-11-29T01:38:42.217956Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_final = pred60.copy()\n",
    "\n",
    "pred_final.prediction = np.mean(\n",
    "    [pred60.prediction, pred80.prediction, pred95.prediction, pred100.prediction],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "pred_final.pred_95_low = np.mean(\n",
    "    [pred60.pred_95_low, pred80.pred_95_low, pred95.pred_95_low, pred100.pred_95_low],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "pred_final.pred_95_high = np.mean(\n",
    "    [\n",
    "        pred60.pred_95_high,\n",
    "        pred80.pred_95_high,\n",
    "        pred95.pred_95_high,\n",
    "        pred100.pred_95_high,\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:42.859487Z",
     "start_time": "2020-11-29T01:38:42.832120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_num</th>\n",
       "      <th>pred_95_low</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_95_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4584.000000</td>\n",
       "      <td>4.584000e+03</td>\n",
       "      <td>4.584000e+03</td>\n",
       "      <td>4.584000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.500000</td>\n",
       "      <td>4.635335e+07</td>\n",
       "      <td>7.330272e+07</td>\n",
       "      <td>1.002521e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.922942</td>\n",
       "      <td>2.323491e+08</td>\n",
       "      <td>3.487585e+08</td>\n",
       "      <td>4.788006e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.050404e+02</td>\n",
       "      <td>9.573269e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.027284e+05</td>\n",
       "      <td>4.085961e+05</td>\n",
       "      <td>5.846164e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.500000</td>\n",
       "      <td>1.592729e+06</td>\n",
       "      <td>3.152802e+06</td>\n",
       "      <td>4.428771e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.250000</td>\n",
       "      <td>1.250872e+07</td>\n",
       "      <td>2.170282e+07</td>\n",
       "      <td>2.916805e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.567764e+09</td>\n",
       "      <td>4.943076e+09</td>\n",
       "      <td>6.318388e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         month_num   pred_95_low    prediction  pred_95_high\n",
       "count  4584.000000  4.584000e+03  4.584000e+03  4.584000e+03\n",
       "mean     11.500000  4.635335e+07  7.330272e+07  1.002521e+08\n",
       "std       6.922942  2.323491e+08  3.487585e+08  4.788006e+08\n",
       "min       0.000000  0.000000e+00  6.050404e+02  9.573269e+02\n",
       "25%       5.750000  2.027284e+05  4.085961e+05  5.846164e+05\n",
       "50%      11.500000  1.592729e+06  3.152802e+06  4.428771e+06\n",
       "75%      17.250000  1.250872e+07  2.170282e+07  2.916805e+07\n",
       "max      23.000000  3.567764e+09  4.943076e+09  6.318388e+09"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:43.506236Z",
     "start_time": "2020-11-29T01:38:43.463771Z"
    }
   },
   "outputs": [],
   "source": [
    "pred60.to_csv(\"../submissions/pred60_noPost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:38:43.935536Z",
     "start_time": "2020-11-29T01:38:43.890745Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_final.to_csv(\"../submissions/pred_final_few.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T20:19:27.228579Z",
     "start_time": "2020-11-28T20:19:27.211469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'brand', 'month_num', 'therapeutic_area', 'presentation',\n",
       "       'month_presentation', 'month_area', 'month_country_num',\n",
       "       'month_presentation_num', 'month_area_num', 'month_month_num',\n",
       "       'n_channels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:32:01.833421Z",
     "start_time": "2020-11-29T01:32:01.613760Z"
    }
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv('../data/gx_merged_lags_monthsD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:32:02.104654Z",
     "start_time": "2020-11-29T01:32:01.872225Z"
    }
   },
   "outputs": [],
   "source": [
    "b = pd.read_csv('../data/gx_merged_lags_monthsM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:32:17.303226Z",
     "start_time": "2020-11-29T01:32:17.295547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12056281.2\n",
       "1        12056281.2\n",
       "2        12056281.2\n",
       "3        12056281.2\n",
       "4        12056281.2\n",
       "            ...    \n",
       "25307    68341440.0\n",
       "25308    68341440.0\n",
       "25309    68341440.0\n",
       "25310    68341440.0\n",
       "25311    68341440.0\n",
       "Name: last_before_3_after_0, Length: 25312, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['last_before_3_after_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:33:12.754316Z",
     "start_time": "2020-11-29T01:33:12.747457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(b['last_before_3_after_0_vMarc'] != b['last_before_3_after_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:31:08.951699Z",
     "start_time": "2020-11-29T01:31:08.935503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25312"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T01:31:13.148553Z",
     "start_time": "2020-11-29T01:31:13.143056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25312,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
